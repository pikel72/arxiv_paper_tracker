#!/usr/bin/env python3
# ArXivè®ºæ–‡è¿½è¸ªä¸åˆ†æå™¨ - ä¸»ç¨‹åº

import argparse
import datetime
import logging
import sys
import time
from concurrent.futures import ThreadPoolExecutor

from config import (
    CATEGORIES, MAX_PAPERS, PAPERS_DIR, RESULTS_DIR,
    PRIORITY_ANALYSIS_DELAY, SECONDARY_ANALYSIS_DELAY,
    PRIORITY_TOPICS, SECONDARY_TOPICS, MAX_THREADS
)
from crawler import get_recent_papers
from analyzer import check_topic_relevance, analyze_paper, extract_pdf_text
from translator import translate_abstract_with_deepseek
from emailer import send_email, format_email_content
from utils import write_to_conclusion, delete_pdf, download_paper

import requests
from models import SimplePaper

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                   handlers=[logging.StreamHandler(sys.stdout)])
logger = logging.getLogger(__name__)

def process_single_paper_task(paper, index, total):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„ä»»åŠ¡å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹"""
    try:
        # çº¿ç¨‹å¯åŠ¨æ—¶ç¨å¾®é”™å¼€ï¼Œé¿å…ç¬é—´å¹¶å‘è¿‡é«˜
        import random
        time.sleep(random.uniform(0, 2))
        
        logger.info(f"æ­£åœ¨å¤„ç†è®ºæ–‡ {index}/{total}: {paper.title}")
        
        # æ£€æŸ¥ä¸»é¢˜ç›¸å…³æ€§å’Œä¼˜å…ˆçº§
        priority, reason = check_topic_relevance(paper)
        
        if priority == 1:
            # é‡ç‚¹å…³æ³¨è®ºæ–‡ï¼šä¸‹è½½PDFå¹¶è¿›è¡Œå®Œæ•´åˆ†æ
            logger.info(f"é‡ç‚¹å…³æ³¨è®ºæ–‡: {paper.title} ({reason})")
            
            pdf_path = download_paper(paper, PAPERS_DIR)
            if pdf_path:
                time.sleep(PRIORITY_ANALYSIS_DELAY)
                analysis = analyze_paper(pdf_path, paper)
                # ä¸åœ¨è¿™é‡Œåˆ é™¤ PDFï¼Œè¿”å› pdf_path ä¾›åç»­ç»Ÿä¸€æ¸…ç†
                return 1, (paper, analysis, pdf_path)
            else:
                # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œé™çº§ä¸ºæ‘˜è¦ç¿»è¯‘
                logger.warning(f"PDFä¸‹è½½å¤±è´¥ï¼Œé™çº§å¤„ç†: {paper.title}")
                time.sleep(SECONDARY_ANALYSIS_DELAY)
                translation = translate_abstract_with_deepseek(paper)
                return 2, (paper, translation)
                
        elif priority == 2:
            # äº†è§£é¢†åŸŸè®ºæ–‡ï¼šåªç¿»è¯‘æ‘˜è¦
            logger.info(f"äº†è§£é¢†åŸŸè®ºæ–‡: {paper.title} ({reason})")
            
            time.sleep(SECONDARY_ANALYSIS_DELAY)
            translation = translate_abstract_with_deepseek(paper)
            return 2, (paper, translation)
            
        else:
            # ä¸ç›¸å…³è®ºæ–‡ï¼šè®°å½•åŸºæœ¬ä¿¡æ¯
            logger.info(f"ä¸ç›¸å…³è®ºæ–‡: {paper.title}")
            
            # ä¸ºä¸ç›¸å…³è®ºæ–‡ç¿»è¯‘æ ‡é¢˜
            time.sleep(SECONDARY_ANALYSIS_DELAY)
            title_translation = translate_abstract_with_deepseek(paper, translate_title_only=True)
            return 0, (paper, reason, title_translation)
    except Exception as e:
        logger.error(f"å¤„ç†è®ºæ–‡å‡ºé”™ {paper.title}: {str(e)}")
        return -1, None

def main():
    parser = argparse.ArgumentParser(
        description="ArXiv è®ºæ–‡è¿½è¸ªä¸åˆ†æå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  æ‰¹é‡æ¨¡å¼ï¼ˆè‡ªåŠ¨æ—¥æœŸï¼‰:
    python src/main.py
  
  æ‰¹é‡æ¨¡å¼ï¼ˆæŒ‡å®šæ—¥æœŸï¼‰:
    python src/main.py --date 2025-12-25
    python src/main.py --date 2025-12-20:2025-12-25
  
  å•è®ºæ–‡åˆ†æï¼ˆarXiv IDï¼‰:
    python src/main.py --arxiv 2401.12345
    python src/main.py --arxiv 2401.12345 -p all
  
  å•PDFåˆ†æï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰:
    python src/main.py --pdf ./papers/some_paper.pdf
    python src/main.py --pdf ./paper.pdf -p 20
  
  ç¼“å­˜ç®¡ç†:
    python src/main.py --cache-stats
    python src/main.py --clear-cache
    python src/main.py --clear-cache analysis
        """
    )
    
    # æ‰¹é‡æ¨¡å¼å‚æ•°
    parser.add_argument('--date', type=str, 
                       help='æŒ‡å®šæŠ“å–æ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD æˆ– YYYY-MM-DD:YYYY-MM-DDï¼ˆæ—¥æœŸèŒƒå›´ï¼‰')
    
    # å•è®ºæ–‡åˆ†æå‚æ•°
    parser.add_argument('--arxiv', type=str, 
                       help='é€šè¿‡ arXiv ID åˆ†æè®ºæ–‡ï¼Œä¾‹å¦‚ 2401.12345')
    parser.add_argument('--pdf', type=str, 
                       help='ç›´æ¥åˆ†ææœ¬åœ° PDF æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-p', '--pages', type=str, default='10', 
                       help='æœ€å¤§PDFæå–é¡µæ•°ï¼Œæ•°å­—æˆ– allï¼ˆå…¨éƒ¨é¡µï¼‰ï¼Œé»˜è®¤ 10')
    
    # å…¼å®¹æ—§å‚æ•°
    parser.add_argument('--single', type=str, 
                       help='[å·²åºŸå¼ƒ] è¯·ä½¿ç”¨ --arxiv ä»£æ›¿')
    
    # ç¼“å­˜ç®¡ç†
    parser.add_argument('--cache-stats', action='store_true', 
                       help='æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--clear-cache', type=str, nargs='?', const='all', 
                       help='æ¸…é™¤ç¼“å­˜ï¼Œå¯é€‰ç±»å‹: classification, analysis, translation, papers, all')
    
    args = parser.parse_args()

    # ç¼“å­˜ç®¡ç†å‘½ä»¤
    if args.cache_stats:
        from cache import get_cache_stats
        stats = get_cache_stats()
        print(f"ğŸ“¦ ç¼“å­˜ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {stats['total']}")
        print(f"   æ€»å¤§å°: {stats['size_mb']} MB")
        print(f"   æŒ‰ç±»å‹:")
        for cache_type, count in stats.get('by_type', {}).items():
            print(f"     - {cache_type}: {count} ä¸ª")
        return
    
    if args.clear_cache:
        from cache import clear_cache
        cache_type = None if args.clear_cache == 'all' else args.clear_cache
        count = clear_cache(cache_type)
        type_str = args.clear_cache if args.clear_cache != 'all' else 'æ‰€æœ‰'
        print(f"ğŸ—‘ï¸ å·²æ¸…é™¤ {count} ä¸ª{type_str}ç¼“å­˜æ–‡ä»¶")
        return

    # è§£æ pages å‚æ•°
    if args.pages.lower() == 'all':
        max_pages = None
    else:
        try:
            max_pages = int(args.pages)
        except Exception:
            max_pages = 10

    # å• PDF åˆ†ææ¨¡å¼ï¼ˆæ–°å¢ï¼‰
    if args.pdf:
        analyze_local_pdf(args.pdf, max_pages=max_pages)
        return

    # å•è®ºæ–‡åˆ†ææ¨¡å¼ï¼ˆé€šè¿‡ arXiv IDï¼‰
    arxiv_id = args.arxiv or args.single  # å…¼å®¹æ—§å‚æ•°
    if arxiv_id:
        if args.single:
            logger.warning("--single å‚æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ --arxiv ä»£æ›¿")
        analyze_single_paper(arxiv_id, max_pages=max_pages)
        return

    # æ‰¹é‡æ¨¡å¼
    start_time = time.time()
    logger.info("å¼€å§‹arXivè®ºæ–‡è·Ÿè¸ª")
    logger.info(f"é…ç½®ä¿¡æ¯:")
    logger.info(f"- æœç´¢ç±»åˆ«: {', '.join(CATEGORIES)}")
    logger.info(f"- æœ€å¤§è®ºæ–‡æ•°: {MAX_PAPERS}")
    logger.info(f"- é‡ç‚¹ä¸»é¢˜æ•°é‡: {len(PRIORITY_TOPICS)}")
    logger.info(f"- äº†è§£ä¸»é¢˜æ•°é‡: {len(SECONDARY_TOPICS)}")
    if args.date:
        logger.info(f"- æŒ‡å®šæ—¥æœŸ: {args.date}")
    
    # è·å–è®ºæ–‡ï¼ˆæ”¯æŒæŒ‡å®šæ—¥æœŸï¼‰
    papers = get_recent_papers(CATEGORIES, MAX_PAPERS, target_date=args.date)
    logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    if not papers:
        logger.info("æ‰€é€‰æ—¶é—´æ®µæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ã€‚é€€å‡ºã€‚")
        return
    
    # å¤„ç†æ¯ç¯‡è®ºæ–‡
    priority_analyses = []  # é‡ç‚¹å…³æ³¨è®ºæ–‡çš„å®Œæ•´åˆ†æ
    secondary_analyses = [] # äº†è§£é¢†åŸŸè®ºæ–‡çš„æ‘˜è¦ç¿»è¯‘
    irrelevant_papers = []  # ä¸ç›¸å…³è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯
    
    logger.info(f"ä½¿ç”¨ {MAX_THREADS} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†è®ºæ–‡...")
    
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [executor.submit(process_single_paper_task, paper, i, len(papers)) 
                   for i, paper in enumerate(papers, 1)]
        
        # ç­‰å¾…ç»“æœ
        for future in futures:
            try:
                p_type, data = future.result()
                if p_type == 1:
                    priority_analyses.append(data)
                elif p_type == 2:
                    secondary_analyses.append(data)
                elif p_type == 0:
                    irrelevant_papers.append(data)
            except Exception as e:
                logger.error(f"è·å–çº¿ç¨‹æ‰§è¡Œç»“æœå‡ºé”™: {str(e)}")
    
    priority_count = len(priority_analyses)
    secondary_count = len(secondary_analyses)
    irrelevant_count = len(irrelevant_papers)
    
    logger.info(f"å¤„ç†å®Œæˆ - é‡ç‚¹å…³æ³¨: {priority_count}ç¯‡, äº†è§£é¢†åŸŸ: {secondary_count}ç¯‡, ä¸ç›¸å…³: {irrelevant_count}ç¯‡")
    
    if not priority_analyses and not secondary_analyses and not irrelevant_papers:
        logger.info("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®ºæ–‡ï¼Œä¸å‘é€é‚®ä»¶ã€‚")
        return
    
    # æå– PDF è·¯å¾„åˆ—è¡¨ï¼Œç”¨äºæœ€åæ¸…ç†
    pdf_paths_to_clean = [data[2] for data in priority_analyses if len(data) > 2 and data[2]]
    
    # è½¬æ¢æ•°æ®æ ¼å¼ï¼šå»æ‰ pdf_pathï¼Œä¿æŒ (paper, analysis) æ ¼å¼ç”¨äºåç»­å¤„ç†
    priority_analyses_clean = [(data[0], data[1]) for data in priority_analyses]
    
    # å°†åˆ†æç»“æœå†™å…¥å¸¦æ—¶é—´æˆ³çš„.mdæ–‡ä»¶
    result_file = write_to_conclusion(priority_analyses_clean, secondary_analyses, irrelevant_papers)
    
    # å‘é€é‚®ä»¶ï¼ŒåŒ…å«é™„ä»¶
    email_content = format_email_content(priority_analyses_clean, secondary_analyses, irrelevant_papers)
    email_success = send_email(email_content, attachment_path=result_file)
    
    if email_success:
        logger.info("é‚®ä»¶å‘é€å®Œæˆ")
    else:
        logger.warning("é‚®ä»¶å‘é€å¯èƒ½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥")
    
    # æ‰€æœ‰æ“ä½œå®Œæˆåï¼Œæœ€åæ¸…ç† PDF æ–‡ä»¶
    if pdf_paths_to_clean:
        logger.info(f"æ¸…ç† {len(pdf_paths_to_clean)} ä¸ª PDF æ–‡ä»¶...")
        for pdf_path in pdf_paths_to_clean:
            delete_pdf(pdf_path)
    
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"ArXivè®ºæ–‡è¿½è¸ªå’Œåˆ†æå®Œæˆï¼Œæ€»è€—æ—¶: {duration:.2f}ç§’")
    logger.info(f"ç»“æœå·²ä¿å­˜è‡³ {result_file.absolute()}")


def fetch_paper_by_id(arxiv_id):
    """é€šè¿‡ arXiv API è·å–å•ç¯‡è®ºæ–‡æ¡ç›®å¹¶è¿”å› SimplePaper å¯¹è±¡æˆ– None"""
    import feedparser
    url = f"https://export.arxiv.org/api/query?id_list={arxiv_id}"
    backoff = 1
    for attempt in range(1, 4):
        try:
            resp = requests.get(url, timeout=30)
            if resp.status_code == 200:
                feed = feedparser.parse(resp.content)
                if not feed.entries:
                    logger.warning(f"æœªæ‰¾åˆ° arXiv ID: {arxiv_id}")
                    return None
                entry = feed.entries[0]
                return SimplePaper(entry)
            else:
                logger.warning(f"å°è¯• {attempt}: arXiv è¿”å›çŠ¶æ€ {resp.status_code}")
        except Exception as e:
            logger.warning(f"å°è¯• {attempt}: è¯·æ±‚ arXiv å‡ºé”™: {str(e)}")

        time.sleep(backoff)
        backoff *= 2

    logger.error(f"ä» arXiv è·å–å…ƒæ•°æ®å¤±è´¥ï¼ˆé‡è¯•3æ¬¡ï¼‰: {arxiv_id}")
    return None


def analyze_single_paper(arxiv_id, max_pages=10):
    """é€šè¿‡ arXiv ID åˆ†æè®ºæ–‡ï¼šè·å–å…ƒæ•°æ®ã€ä¸‹è½½PDFã€è°ƒç”¨AIåˆ†æå¹¶å†™å…¥ç»“æœã€‚"""
    start_time = time.time()
    logger.info(f"å¼€å§‹å•è®ºæ–‡åˆ†æ: {arxiv_id}")

    paper = fetch_paper_by_id(arxiv_id)
    if not paper:
        logger.error(f"æœªèƒ½è·å–åˆ° arXiv è®ºæ–‡: {arxiv_id}")
        return

    # ä¸‹è½½ PDF
    pdf_path = download_paper(paper, PAPERS_DIR)
    if not pdf_path:
        logger.error("PDF ä¸‹è½½å¤±è´¥ï¼Œç»ˆæ­¢åˆ†æ")
        return

    # è°ƒç”¨åˆ†æå‡½æ•°ï¼ˆå¤ç”¨ analyzer.analyze_paperï¼‰
    analysis = analyze_paper(pdf_path, paper, max_pages=max_pages)

    # ç”¨å•è®ºæ–‡ä¸“ç”¨è¾“å‡ºå‡½æ•°ç”Ÿæˆ Markdown æ–‡ä»¶
    safe_id = arxiv_id.replace('/', '_')
    now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    custom_filename = f"arxiv_{safe_id}_{now}.md"
    from utils import write_single_analysis
    result_file = write_single_analysis(paper, analysis, filename=custom_filename)
    
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"å•è®ºæ–‡åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶: {duration:.2f}ç§’ï¼Œç»“æœä¿å­˜è‡³: {result_file}")

    # å¯é€‰æ‹©åˆ é™¤ PDF
    delete_pdf(pdf_path)


def analyze_local_pdf(pdf_path, max_pages=10):
    """ç›´æ¥åˆ†ææœ¬åœ° PDF æ–‡ä»¶ï¼Œä¸ä¾èµ– arXiv å…ƒæ•°æ®"""
    from pathlib import Path
    from analyzer import analyze_pdf_only
    
    start_time = time.time()
    pdf_path = Path(pdf_path)
    
    if not pdf_path.exists():
        logger.error(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return
    
    logger.info(f"å¼€å§‹åˆ†ææœ¬åœ° PDF: {pdf_path}")
    
    # è°ƒç”¨çº¯ PDF åˆ†æå‡½æ•°
    analysis = analyze_pdf_only(str(pdf_path), max_pages=max_pages)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    output_filename = f"pdf_{pdf_path.stem}_{now}.md"
    output_path = RESULTS_DIR / output_filename
    
    # ä»åˆ†æç»“æœä¸­æå–æ ‡é¢˜ä¿¡æ¯
    chinese_title = ""
    english_title = ""
    authors = ""
    
    if analysis:
        for line in analysis.split('\n'):
            if line.startswith("**ä¸­æ–‡æ ‡é¢˜**:"):
                chinese_title = line.replace("**ä¸­æ–‡æ ‡é¢˜**:", "").strip()
            elif line.startswith("**è‹±æ–‡æ ‡é¢˜**:"):
                english_title = line.replace("**è‹±æ–‡æ ‡é¢˜**:", "").strip()
            elif line.startswith("**ä½œè€…**:"):
                authors = line.replace("**ä½œè€…**:", "").strip()
    
    # å†™å…¥ Markdown æ–‡ä»¶
    datetime_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    from config import AI_MODEL
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"---\n")
        f.write(f"title: \"{chinese_title if chinese_title else pdf_path.stem}\"\n")
        f.write(f"date: {datetime_str}\n")
        f.write(f"source: local_pdf\n")
        if authors:
            f.write(f"description: {authors}\n")
        f.write(f"pdf_file: {pdf_path.name}\n")
        f.write(f"ai_model: {AI_MODEL}\n")
        f.write(f"---\n\n")
        
        if chinese_title:
            f.write(f"# {chinese_title}\n\n")
        if english_title:
            f.write(f"**{english_title}**\n\n")
        if authors:
            f.write(f"**ä½œè€…**: {authors}\n\n")
        
        f.write(f"---\n\n")
        f.write(f"## è¯¦ç»†åˆ†æ\n\n{analysis}\n")
    
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"PDF åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶: {duration:.2f}ç§’")
    logger.info(f"ç»“æœä¿å­˜è‡³: {output_path.absolute()}")


if __name__ == "__main__":
    main()
